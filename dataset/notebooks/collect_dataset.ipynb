{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import hyperopt.pyll.stochastic\n",
    "import numpy as np\n",
    "from fedot.api.main import Fedot\n",
    "from fedot.core.pipelines.adapters import PipelineAdapter\n",
    "from fedot.core.pipelines.pipeline import Pipeline\n",
    "from fedot.core.pipelines.pipeline_composer_requirements import PipelineComposerRequirements\n",
    "from fedot.core.pipelines.pipeline_node_factory import PipelineOptNodeFactory\n",
    "from fedot.core.pipelines.random_pipeline_factory import RandomPipelineFactory\n",
    "from fedot.core.pipelines.tuning.search_space import PipelineSearchSpace\n",
    "from fedot.core.repository.operation_types_repository import get_operations_for_task\n",
    "from fedot.core.repository.tasks import TaskTypesEnum, Task\n",
    "from golem.core.dag.graph_verifier import GraphVerifier\n",
    "from golem.core.dag.graph_verifier import VerificationError\n",
    "from golem.core.dag.verification_rules import DEFAULT_DAG_RULES\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class PipelineGenerator:\n",
    "    def __init__(self):\n",
    "        task = Task(TaskTypesEnum.classification)\n",
    "        ops = get_operations_for_task(task)\n",
    "        rules_for_constraint = tuple(DEFAULT_DAG_RULES)\n",
    "        self.adapter = PipelineAdapter()\n",
    "        verifier = GraphVerifier(rules_for_constraint, self.adapter)\n",
    "        self.pipeline_requirements = PipelineComposerRequirements(primary=ops, secondary=ops)\n",
    "        node_factory = PipelineOptNodeFactory(self.pipeline_requirements)\n",
    "        self.random_pipeline_factory = RandomPipelineFactory(verifier, node_factory)\n",
    "        self.parameters_per_operation = PipelineSearchSpace().parameters_per_operation\n",
    "\n",
    "    def get_random_pipeline(self, randomize_hyperparameters=True):\n",
    "        graph = self.random_pipeline_factory(self.pipeline_requirements)\n",
    "        pipeline = self.adapter._restore(graph)\n",
    "        if randomize_hyperparameters:\n",
    "            pipeline = self.randomize_hyperparameters(pipeline)\n",
    "        return pipeline\n",
    "\n",
    "    def randomize_hyperparameters(self, pipeline: Pipeline) -> Pipeline:\n",
    "        for i in range(len(pipeline.nodes)):\n",
    "            node = pipeline.nodes[i]\n",
    "            operation_name = node.name\n",
    "            new_parameters = {}\n",
    "            try:\n",
    "                parameters_per_operation = self.parameters_per_operation[operation_name]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            for hp_name, hp_space in parameters_per_operation.items():\n",
    "                function, args = hp_space\n",
    "                space = function(hp_name, *args)\n",
    "                new_parameters[hp_name] = hyperopt.pyll.stochastic.sample(space)\n",
    "            pipeline.nodes[i].parameters = new_parameters\n",
    "        return pipeline\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.get_random_pipeline()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class FittedPipelineGenerator:\n",
    "    def __init__(self):\n",
    "        self.save_dir = \"../pipeline_dataset\"\n",
    "        self.features = np.load(\"../synthetic_dataset/features.npy\")\n",
    "        self.target = np.load(\"../synthetic_dataset/target.npy\")\n",
    "        self.api = Fedot(\"classification\", logging_level=logging.CRITICAL)\n",
    "        self.pipeline_generator = PipelineGenerator()\n",
    "        self.caught_errors = []\n",
    "\n",
    "    def get_fitted_pipeline(self) -> Tuple[Pipeline, Dict[str, float]]:\n",
    "        fitted = False\n",
    "        while not fitted:\n",
    "            pipeline = self.pipeline_generator()\n",
    "            try:\n",
    "                with HiddenPrints():\n",
    "                    fitted_pipeline = self.api.fit(self.features, self.target, pipeline)\n",
    "                fitted = True\n",
    "            except VerificationError:\n",
    "                continue\n",
    "        self.api.predict(self.features)\n",
    "        try:\n",
    "            metrics = self.api.get_metrics(self.target)\n",
    "        except Exception as e:  # TODO: specify error\n",
    "            self.caught_errors.append(e)\n",
    "            return self.get_fitted_pipeline()\n",
    "        return fitted_pipeline, metrics\n",
    "\n",
    "    def save_sample(self, pipeline: Pipeline, metrics: Dict[str, float]):\n",
    "        json_object, dict_fitted_operations = pipeline.save(os.path.join(self.save_dir, \"pipelines\"))\n",
    "        sample_name = dict_fitted_operations[\"operation_0\"].split(\"\\\\\")[-3]  # TODO: fix hardcode for Windows\n",
    "        metrics_dir = os.path.join(self.save_dir, \"metrics\", sample_name)\n",
    "        if not os.path.exists(metrics_dir):\n",
    "            os.mkdir(metrics_dir)\n",
    "        with open(os.path.join(metrics_dir, \"metrics.pickle\"), \"wb\") as f:\n",
    "            pickle.dump(metrics, f)\n",
    "\n",
    "    def generate(self):\n",
    "        for _ in tqdm(range(10000)):\n",
    "            fitted_pipeline, metrics = self.get_fitted_pipeline()\n",
    "            self.save_sample(fitted_pipeline, metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "fpg = FittedPipelineGenerator()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fpg.generate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
